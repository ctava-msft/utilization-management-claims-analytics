"""Policy Markdown parser.

Parses structured Markdown policy documents (generated by generate_policy_md)
back into structured JSON rule dictionaries.

Spec: Feature — Policy parser to extract CPT/rules from generated Markdown + round-trip checks
"""

from __future__ import annotations

import json
import re
from pathlib import Path

# Matches CPT-prefixed codes (e.g. CPT-99201) or bare 4–5-digit CPT codes (e.g. 99201)
_CPT_PATTERN = re.compile(r"\bCPT-[\w\d]+\b|\b[0-9]{4,5}[A-Z]?\b")

# Matches the H1 title line: # Prior Authorization Policy: <code>
_TITLE_RE = re.compile(r"^#\s+Prior Authorization Policy:\s*(.+)$", re.MULTILINE)

# Matches: **Site of Service (ClaimType):** <value>
_SITE_OF_SERVICE_RE = re.compile(
    r"\*\*Site of Service \(ClaimType\):\*\*\s*(.+)$", re.MULTILINE
)

# Markdown section splitter: ## N. Title
_SECTION_RE = re.compile(r"^##\s+\d+\.\s+.+$", re.MULTILINE)


def _extract_section(md: str, section_title: str) -> str:
    """Return the body text of the first section whose heading contains *section_title*.

    The body spans from the line after the heading to the next ``##`` heading or
    the end of the document.

    Args:
        md: Full Markdown string.
        section_title: Substring to match against section heading (case-insensitive).

    Returns:
        Section body text, stripped of leading/trailing whitespace.
        Empty string if the section is not found.
    """
    sections = _SECTION_RE.split(md)
    headings = _SECTION_RE.findall(md)

    for heading, body in zip(headings, sections[1:], strict=False):
        if section_title.lower() in heading.lower():
            return body.strip()
    return ""


def _extract_bullet_list(text: str) -> list[str]:
    """Return a list of strings from Markdown bullet-list items in *text*."""
    items: list[str] = []
    for line in text.splitlines():
        stripped = line.strip()
        if stripped.startswith("- ") or stripped.startswith("* "):
            items.append(stripped[2:].strip())
    return items


def _extract_dx_codes(section_body: str) -> list[str]:
    """Extract Dx-style codes from a Markdown section body.

    Codes are assumed to appear as ``  - <code>`` bullet items.  Any item that
    equals ``N/A`` is excluded.

    Args:
        section_body: Text of the Diagnosis Context section.

    Returns:
        List of diagnosis code strings.
    """
    codes: list[str] = []
    for line in section_body.splitlines():
        stripped = line.strip()
        if stripped.startswith("- ") or stripped.startswith("* "):
            code = stripped[2:].strip()
            if code and code.upper() != "N/A":
                codes.append(code)
    return codes


def parse_policy_markdown(md: str, policy_id: str = "") -> dict:
    """Parse a structured Markdown policy document into a rule dictionary.

    Args:
        md: Markdown string as generated by
            :func:`um_claims.policy.generate_policy_md.generate_policy_markdown`.
        policy_id: Optional identifier for the policy (e.g. the stem of the
            source filename).  If omitted the procedure code extracted from the
            H1 title is used.

    Returns:
        Dictionary with the following keys:

        - ``policy_id`` (str): identifier for this policy.
        - ``covered_cpt_codes`` (list[str]): CPT-like tokens found in the document.
        - ``site_of_service`` (str): value from the ClaimType line.
        - ``diagnosis_constraints`` (list[str]): Dx codes listed in the Diagnosis
          Context section.
        - ``documentation_requirements`` (list[str]): bullet items from the
          Documentation Requirements section.
    """
    # --- policy_id / procedure code from title ---
    title_match = _TITLE_RE.search(md)
    procedure_code = title_match.group(1).strip() if title_match else ""
    resolved_policy_id = policy_id or procedure_code

    # --- covered CPT codes ---
    # Collect all CPT-like tokens from the full document; deduplicate preserving order.
    seen: set[str] = set()
    covered_cpt_codes: list[str] = []
    for token in _CPT_PATTERN.findall(md):
        if token not in seen:
            seen.add(token)
            covered_cpt_codes.append(token)

    # --- site of service ---
    sos_match = _SITE_OF_SERVICE_RE.search(md)
    site_of_service = sos_match.group(1).strip() if sos_match else ""

    # --- diagnosis constraints ---
    dx_section = _extract_section(md, "Diagnosis Context")
    diagnosis_constraints = _extract_dx_codes(dx_section)

    # --- documentation requirements ---
    doc_section = _extract_section(md, "Documentation Requirements")
    documentation_requirements = _extract_bullet_list(doc_section)

    return {
        "policy_id": resolved_policy_id,
        "covered_cpt_codes": covered_cpt_codes,
        "site_of_service": site_of_service,
        "diagnosis_constraints": diagnosis_constraints,
        "documentation_requirements": documentation_requirements,
    }


def parse_policies_dir(
    policy_dir: str | Path,
    output: str | Path = "output/policies.jsonl",
) -> Path:
    """Parse all ``.md`` files in *policy_dir* and write a JSONL file.

    Each line of the output file is a JSON object produced by
    :func:`parse_policy_markdown`, augmented with a ``source_file`` key.

    Args:
        policy_dir: Directory containing Markdown policy files.
        output: Path for the output JSONL file.

    Returns:
        :class:`~pathlib.Path` of the written JSONL file.

    Raises:
        FileNotFoundError: If *policy_dir* does not exist.
    """
    in_dir = Path(policy_dir)
    if not in_dir.is_dir():
        raise FileNotFoundError(f"Policy directory not found: {in_dir}")

    out_path = Path(output)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    md_files = sorted(in_dir.glob("*.md"))
    records: list[dict] = []
    for md_file in md_files:
        md_text = md_file.read_text(encoding="utf-8")
        record = parse_policy_markdown(md_text, policy_id=md_file.stem)
        record["source_file"] = str(md_file.name)
        records.append(record)

    with out_path.open("w", encoding="utf-8") as fh:
        for rec in records:
            fh.write(json.dumps(rec) + "\n")

    return out_path
